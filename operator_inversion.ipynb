{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference in a neural emulation model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look, we can do approximate probabilistic inference with a complicated neural network! Here we will use a 2-million parameter neural net to generate a an approximate likelihood for inference in a model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import sqrt, ceil, floor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.functional import F \n",
    "\n",
    "from src.graphs import fno_graph\n",
    "from src.plots import img_plot, multi_img_plot_time, multi_heatmap\n",
    "import h5py \n",
    "\n",
    "# device = torch.device('cuda')\n",
    "device = torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data\n",
    "\n",
    "Included in this repository is a minimalist dataset of snapshots from a [Navier-Stokes](https://en.wikipedia.org/wiki/Navier%E2%80%93Stokes_equations) simulator.\n",
    "\n",
    "This dataset is packed as as $(b, x, y, t)$ i.e. batch first, coordinates in the middle, time last. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obs(data, t=0, n_steps=2, y=False, b0=0, b1=1):\n",
    "    \"\"\"\n",
    "    helper function to parse the data into chunks with the correct size and names for the model\n",
    "    \"\"\"\n",
    "    x = data['u'][b0:b1, ..., t:t+n_steps]\n",
    "    latent = data['f'][b0:b1, ...]\n",
    "    obs= {\n",
    "        'x': x,\n",
    "        'latent': latent\n",
    "    }\n",
    "    if y:\n",
    "        obs['y'] = data['u'][b0:b1, ..., t+n_steps]\n",
    "\n",
    "    return obs\n",
    "\n",
    "def dict_as_tensor(d, device=device):\n",
    "    \"\"\"\n",
    "    Send a dict of arrays to pytorch tensors.\n",
    "    it was faster to write this function than to search the docs for it\n",
    "    \"\"\"\n",
    "    return {k: torch.as_tensor(v).to(device) for k, v in d.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_data = np.load('./data/grf_forcing_single.npz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = h5py.File('data/grf_forcing_mini_1.h5', 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise the data\n",
    "\n",
    "First we look at the snapshots of the simulation. First the vorticity field $x(t)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_img_plot_time(get_obs(single_data, 0, -1)['x'], n_cols=4, interval=5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we look at the _latent forcing_, which is a time-invariant field that perturbs the dynamics of $x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_plot(get_obs(single_data, 0, -1)['latent']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process predictor model\n",
    "\n",
    "Running a PDE simulator is expensive, so we have trained a neural network which can hopefully reproduce these observations, by giving it a data set of 1000 simulations made up of many snapshots. The details of the network are on [Zongyi Li’s blog](https://zongyi-li.github.io/blog/2020/fourier-pde/).\n",
    "\n",
    "NB the data we have here is from the validation set — the neural network has not seen this data during the training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.nn_modules.fourier_2d_generic import SimpleBlock2dGeneric\n",
    "\n",
    "pp_state_dict = torch.load(\n",
    "    './models/fno_forward.ckpt',\n",
    "    map_location=device\n",
    ")\n",
    "process_predictor = SimpleBlock2dGeneric(\n",
    "    modes1=16,\n",
    "    width=24,\n",
    "    n_layers=4,\n",
    "    n_history=2,\n",
    "    param=False,\n",
    "    forcing=False,\n",
    "    latent=True,\n",
    ")\n",
    "process_predictor.load_state_dict(\n",
    "    pp_state_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model maps the vorticity field $x$ at two previous timesteps and the latent forcing field to the vorticity field at the next timestep.\n",
    "We think of the neural network weights $\\theta$ as (some approximation to) the parameters of the model, and we learn them by minimising some predictive error.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fno_graph(obs=\"fwd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can repeatedly invoke this to push the model forward in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_forward(x, latent, n_horizon=1, n_steps=2):\n",
    "    x = torch.as_tensor(x).to(device)\n",
    "    latent = torch.as_tensor(latent).to(device)\n",
    "    process_predictor.to(device)\n",
    "    for i in range(n_horizon):\n",
    "        pred = process_predictor({'x': x[...,-(n_steps):], 'latent': latent})['forecast']\n",
    "        x = torch.cat((x, pred), dim=-1)\n",
    "    return x[...,-n_horizon:]\n",
    "\n",
    "obs = get_obs(single_data, 0, 10,)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = predict_forward(obs['x'], obs['latent'], n_horizon=50, n_steps=2)\n",
    "\n",
    "multi_img_plot_time(pred.cpu().numpy(), n_cols=5, interval=5);\n",
    "## How the teaser graphic in the README was generated:\n",
    "# plt.savefig('./fno_forward_predict_sheet.jpg');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inversion by GD\n",
    "\n",
    "Now we want to solve an inverse problem with this model: given some observations of $x(t), x(t-1), x(t-2)$, what is the best guess for the latent forcing field?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fno_graph(obs=\"inverse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can in fact predict and do inversion through the PDE equations directly, but this is _slow_ (hours), so we use the neural network to solve this problem directly.\n",
    "Firstly, we can do this directly as pure predictive error minimisation problem. Forget the probabilistic interpretation of this problem and just minimise some loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RasterLatent(nn.Module):\n",
    "    def __init__(self,\n",
    "            process_predictor: \"nn.Module\",\n",
    "            dims = (256,256),\n",
    "            latent_dims = None,\n",
    "            interpolation = 'bilinear',\n",
    "            n_batch: int=1):\n",
    "        super().__init__()\n",
    "        self.dims = dims\n",
    "        if latent_dims is None:\n",
    "            latent_dims = dims\n",
    "        self.latent_dims = latent_dims\n",
    "        self.interpolation = interpolation\n",
    "        self.process_predictor = process_predictor\n",
    "        ## Do not fit the process predictor weights\n",
    "        process_predictor.train(False)\n",
    "        for param in self.process_predictor.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        self.latent = nn.Parameter(\n",
    "            torch.zeros(\n",
    "                (n_batch, *latent_dims),\n",
    "                dtype=torch.float32\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def get_latent(self):\n",
    "        if self.latent_dims==self.dims:\n",
    "            return self.latent\n",
    "        return F.interpolate(\n",
    "            self.latent.unsqueeze(1),\n",
    "            self.dims,\n",
    "            mode=self.interpolation\n",
    "        ).squeeze(1)  #squeeze/unsqueeze is because of weird interpolate semantics\n",
    "\n",
    "    def weights_init(self, scale=0.005):\n",
    "        self.latent.data.normal_(0.0, scale)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        batch = dict(**batch)\n",
    "        batch['latent'] = self.get_latent()\n",
    "        return self.process_predictor(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(\n",
    "        batch,\n",
    "        model,\n",
    "        optimizer,\n",
    "        n_iter: int=20,\n",
    "        check_int: int=1,\n",
    "        clip_val = None,\n",
    "        init_scale=0.1):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    model.weights_init(init_scale)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    big_loss_fn = nn.MSELoss(reduction='none')\n",
    "    scale = loss_fn(torch.zeros_like(batch['latent']), batch['latent']).item()\n",
    "    for i in range(n_iter):\n",
    "        # Compute prediction error\n",
    "        pred = model(batch)\n",
    "        loss = loss_fn(pred['forecast'], batch['y'])\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        if clip_val is not None:\n",
    "            for group in optimizer.param_groups:\n",
    "                torch.nn.utils.clip_grad_value_(group[\"params\"], clip_val)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % check_int == 0 or i==n_iter-1:\n",
    "            with torch.no_grad():\n",
    "                loss_v = loss.item()\n",
    "                # batchwise error\n",
    "                big_error = big_loss_fn(model.get_latent(), batch['latent']).mean(dim=(1,2))\n",
    "                big_relerr = torch.sqrt(big_error/scale)\n",
    "                error = big_error.mean().item()\n",
    "                relerr = sqrt(big_relerr.mean().item())\n",
    "                print(\n",
    "                    f\"loss: {loss:.3e}, error: {error:.3e}, relerror: {relerr:.3e} [{i:>5d}/{n_iter:>5d}]\")\n",
    "\n",
    "                target =  batch['latent'][0, :, :].cpu().numpy()\n",
    "                est =  model.get_latent()[0, :, :].cpu().numpy()\n",
    "                err_heatmap = target - est\n",
    "                multi_heatmap(\n",
    "                    [target, est, err_heatmap],\n",
    "                    [\"Target\", \"Estimate\", \"Error\"])\n",
    "                plt.show();\n",
    "                plt.close(\"all\");\n",
    "\n",
    "    return loss_v, error, relerr, scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RasterLatent(\n",
    "    process_predictor,\n",
    "    dims=obs['x'].shape[1:3],\n",
    "    latent_dims=(256,256),\n",
    "    n_batch=1)\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=0.0025,\n",
    "    weight_decay=0.0)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "fit(\n",
    "    dict_as_tensor(get_obs(single_data,t=10,n_steps=2,y=True)),\n",
    "    model,\n",
    "    optimizer,\n",
    "    n_iter=50,\n",
    "    check_int=10,\n",
    "    clip_val=None,\n",
    "    init_scale=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fit_reg(\n",
    "        batch,\n",
    "        model,\n",
    "        optimizer,\n",
    "        n_iter:int=20,\n",
    "        check_int:int=1,\n",
    "        clip_val = None,\n",
    "        pen_1: float = 0.0,\n",
    "        stop_on_truth: bool = False,\n",
    "        diminishing_returns=1.1,\n",
    "        init_scale=0.1):\n",
    "    model.train()\n",
    "    model.weights_init(init_scale)\n",
    "    prev_loss_v = 10^5\n",
    "    prev_error = 10^5\n",
    "    big_losses = []\n",
    "    loss_fn = nn.MSELoss()\n",
    "    big_loss_fn = nn.MSELoss(reduction='none')\n",
    "    big_scale = big_loss_fn(torch.zeros_like(batch['latent']), batch['latent']).mean((1,2))\n",
    "    scale = loss_fn(torch.zeros_like(batch['latent']), batch['latent']).item()\n",
    "    for i in range(n_iter):\n",
    "        # Compute prediction error\n",
    "        pred = model(batch)\n",
    "        loss = loss_fn(pred['forecast'], batch['y'])\n",
    "\n",
    "        if pen_1 > 0.0:\n",
    "            print(\"Penalty 1:\", pen_1)\n",
    "            loss += model.latent.diff(dim =-1).square().mean() * pen_1\n",
    "            loss += model.latent.diff(dim =-2).square().mean() * pen_1\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        if clip_val is not None:\n",
    "            for group in optimizer.param_groups:\n",
    "                torch.nn.utils.clip_grad_value_(group[\"params\"], clip_val)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % check_int == 0 or i==n_iter-1:\n",
    "            with torch.no_grad():\n",
    "                # recalc without penalties\n",
    "                loss_v = loss_fn(pred['forecast'], batch['y']).item()\n",
    "                if loss_v > diminishing_returns * prev_loss_v and i> 15:\n",
    "                    print(\"Early stopping at optimum\")\n",
    "                    break\n",
    "                prev_loss_v = loss_v\n",
    "                error = loss_fn(model.latent, batch['latent']).item()\n",
    "                if error > diminishing_returns * prev_error and stop_on_truth:\n",
    "                    print(\"Early stopping at minimum prediction error\")\n",
    "                    break\n",
    "                prev_error = error\n",
    "                relerr = sqrt(error/scale)\n",
    "                \n",
    "                big_loss_v = big_loss_fn(pred['forecast'], batch['y']).mean((1,2,3))\n",
    "                print(big_loss_v.shape)\n",
    "                big_error = big_loss_fn(model.latent, batch['latent']).mean((1,2))\n",
    "                big_relerr = torch.sqrt(big_error/scale)\n",
    "                big_losses.append(dict(\n",
    "                    big_loss=big_loss_v.detach().cpu().numpy(),\n",
    "                    big_error = big_error.detach().cpu().numpy(),\n",
    "                    relerr=big_relerr.detach().cpu().numpy()\n",
    "                ))\n",
    "\n",
    "                print(\n",
    "                    f\"loss: {loss:.3e}, error: {error:.3e}, relerror: {relerr:.3e} [{i:>5d}/{n_iter:>5d}]\")\n",
    "\n",
    "                target =  batch['latent'][0, :, :].cpu().numpy()\n",
    "                est =  model.get_latent()[0, :, :].cpu().numpy()\n",
    "                err_heatmap = target - est\n",
    "                multi_heatmap(\n",
    "                    [target, est, err_heatmap],\n",
    "                    [\"Target\", \"Estimate\", \"Error\"])\n",
    "                plt.show();\n",
    "                plt.close(\"all\");\n",
    "\n",
    "    loss_v = loss.item()\n",
    "    error = loss_fn(model.latent, batch['latent']).item()\n",
    "    scale = loss_fn(torch.zeros_like(batch['latent']), batch['latent']).item()\n",
    "    relerr = sqrt(error/scale)\n",
    "    print(\n",
    "        f\"loss: {loss:.3e}, error: {error:.3e}, relerror: {relerr:.3e} scale: {scale:.3e}[{i:>5d}/{n_iter:>5d}]\")\n",
    "\n",
    "    return loss_v, error, relerr, scale, big_losses, big_scale.detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RasterLatent(\n",
    "    process_predictor,\n",
    "    dims=obs['x'].shape[1:3],\n",
    "    latent_dims=(256,256),\n",
    "    n_batch=1)\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=0.0025,\n",
    "    weight_decay=0.0)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "fit_reg(\n",
    "    dict_as_tensor(get_obs(single_data,t=0,n_steps=2,y=True)),\n",
    "    model,\n",
    "    optimizer,\n",
    "    n_iter=50,\n",
    "    check_int=10,\n",
    "    clip_val=None,\n",
    "    init_scale=0.01,\n",
    "    pen_1=30,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "887521cc036c2176fc4c7c5fad660bcf5f9a9c2cadfc49851d28bf162a40070d"
  },
  "kernelspec": {
   "display_name": "Cadabra2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
